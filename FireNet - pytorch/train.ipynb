{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from model import FireNet\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FireNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_PATH='Dataset'\n",
    "CATEGORIES = ['Fire', 'NoFire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "def create_training_data():\n",
    "    training_data = []\n",
    "    for category in CATEGORIES:  \n",
    "\n",
    "        path = os.path.join(TRAINING_PATH,category) \n",
    "        class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 0=C 1=O\n",
    "\n",
    "        for img in tqdm(os.listdir(path)):  # iterate over each image\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img))  # convert to array\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
    "                training_data.append([new_array, class_num])  # add this to our training_data\n",
    "            except Exception as e:  # in the interest in keeping the output clean...\n",
    "                pass\n",
    "              \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1125/1125 [00:15<00:00, 72.20it/s]\n",
      "100%|██████████| 1302/1302 [00:33<00:00, 38.63it/s]\n"
     ]
    }
   ],
   "source": [
    "training_data = create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSet(Dataset):\n",
    "    def __init__(self, training_dir, transform=None):\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        pass\n",
    "    def __getitem__(self, idx):\n",
    "        sample = None\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img = io.imread(os.path.join(TRAINING_PATH, CATEGORIES[idx]))\n",
    "        print(img)\n",
    "\n",
    "        return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
